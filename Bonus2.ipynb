{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus 2: Parallel Computing\n",
    "\n",
    "### Name: [Gopi Miyani]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. You will do the following:\n",
    "\n",
    "1. Read the lecture note and the corresponding research papers.\n",
    "(https://github.com/wangshusen/DeepLearning/blob/master/LectureNotes/Parallel/Parallel.pdf)\n",
    "\n",
    "\n",
    "2. Implement federated averaging (easier) and/or decentralized optimization (harder). 2 bonus points for each. If you do both, submit them as two separate IPYNB files.\n",
    "\n",
    "\n",
    "3. Data: use the data and template of Bonus 1.\n",
    "\n",
    "\n",
    "4. Requirement for federated averaging:\n",
    "\n",
    "Use 4 worker nodes.\n",
    "Use GD and SGD (with batch size b=1).\n",
    "Set the number of local iterations to q=1 and 8.\n",
    "Plot (training) objective function against epochs. Note that one epoch means every sample is visited exactly once.\n",
    "Note also that there are 4 lines in your plot. (GD with q=1, GD with q=8, SGD with q=1, SGD with q=8.)\n",
    "\n",
    "5. Requirement for decentralized optimization:\n",
    "\n",
    "Use 7 worker nodes; the connection must be the same as the figure in the lecture note.\n",
    "Use GD.\n",
    "Try weighted averaging (give a worker's own parameter a higher weight than its neighbors' parameters) and simple averaging (everyone has the same weight).\n",
    "Plot (training) objective function against epochs. \n",
    "Note that there are at least 2 lines in your plot. (Corresponding to the weighting schemes.)\n",
    " \n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data processing\n",
    "\n",
    "- Download the Diabete dataset from https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/diabetes\n",
    "- Load the data using sklearn.\n",
    "- Preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x: (768, 8)\n",
      "Shape of y: (768,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy\n",
    "\n",
    "x_sparse, y = datasets.load_svmlight_file('diabetes')\n",
    "x = x_sparse.todense()\n",
    "\n",
    "print('Shape of x: ' + str(x.shape))\n",
    "print('Shape of y: ' + str(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Partition to training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (615, 8)\n",
      "Shape of x_test: (153, 8)\n",
      "Shape of y_train: (615, 1)\n",
      "Shape of y_test: (153, 1)\n"
     ]
    }
   ],
   "source": [
    "# partition the data to training and test sets\n",
    "n = x.shape[0]\n",
    "n_train = int(numpy.ceil(n * 0.8))\n",
    "n_test = n - n_train\n",
    "\n",
    "rand_indices = numpy.random.permutation(n)\n",
    "train_indices = rand_indices[0:n_train]\n",
    "test_indices = rand_indices[n_train:n]\n",
    "\n",
    "x_train = x[train_indices, :]\n",
    "x_test = x[test_indices, :]\n",
    "y_train = y[train_indices].reshape(n_train, 1)\n",
    "y_test = y[test_indices].reshape(n_test, 1)\n",
    "\n",
    "print('Shape of x_train: ' + str(x_train.shape))\n",
    "print('Shape of x_test: ' + str(x_test.shape))\n",
    "print('Shape of y_train: ' + str(y_train.shape))\n",
    "print('Shape of y_test: ' + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the standardization to trainsform both training and test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mean = \n",
      "[[ 0.06292817  0.01063182 -0.00699233 -0.06477095 -0.03569728  0.05473539\n",
      "   0.12394643  0.17639928]]\n",
      "test std = \n",
      "[[1.04738308 0.93685409 1.12869021 1.09330051 0.95301273 1.17454921\n",
      "  0.98710941 1.1021825 ]]\n"
     ]
    }
   ],
   "source": [
    "# Standardization\n",
    "import numpy\n",
    "\n",
    "# calculate mu and sig using the training set\n",
    "d = x_train.shape[1]\n",
    "mu = numpy.mean(x_train, axis=0).reshape(1, d)\n",
    "sig = numpy.std(x_train, axis=0).reshape(1, d)\n",
    "\n",
    "# transform the training features\n",
    "x_train = (x_train - mu) / (sig + 1E-6)\n",
    "\n",
    "# transform the test features\n",
    "x_test = (x_test - mu) / (sig + 1E-6)\n",
    "\n",
    "print('test mean = ')\n",
    "print(numpy.mean(x_test, axis=0))\n",
    "\n",
    "print('test std = ')\n",
    "print(numpy.std(x_test, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Add a dimension of all ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (615, 9)\n",
      "Shape of x_test: (153, 9)\n"
     ]
    }
   ],
   "source": [
    "n_train, d = x_train.shape\n",
    "x_train = numpy.concatenate((x_train, numpy.ones((n_train, 1))), axis=1)\n",
    "\n",
    "n_test, d = x_test.shape\n",
    "x_test = numpy.concatenate((x_test, numpy.ones((n_test, 1))), axis=1)\n",
    "\n",
    "print('Shape of x_train: ' + str(x_train.shape))\n",
    "print('Shape of x_test: ' + str(x_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Logistic regression model\n",
    "\n",
    "The objective function is $Q (w; X, y) = \\frac{1}{n} \\sum_{i=1}^n \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Worker Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker:\n",
    "    def __init__(self , x, y): \n",
    "        self.x = x # s—by—d local feature matrix \n",
    "        self.y = y # s—by-1 local label matrix \n",
    "        self.s = x. shape [0] # number of local samples \n",
    "        self.d = x. shape [1] # number of features \n",
    "        self.w = numpy. zeros ((d, 1)) # d—by-1 model parameter vector \n",
    "\n",
    "    # Set the model parameters to the latest \n",
    "    def set_param(self ,w): \n",
    "        self.w = w \n",
    "\n",
    "    # Compute the local loss \n",
    "    def loss(self): \n",
    "        yx = numpy.multiply(self.y, self.x) # s—by—d matrix \n",
    "        yxw = numpy.dot(yx, self.w) # s—by-1 matrix \n",
    "        vecl = numpy.exp(-(yxw)) # s—by-1 matrix \n",
    "        vec2 = numpy.log (1 + vecl) # s—by-1 matrix \n",
    "        return numpy.sum(vec2) # loss function \n",
    "\n",
    "    # Compute the local gradient \n",
    "    def gradient(self,q): \n",
    "        yx = numpy.multiply (self.y, self.x) # s—by—d matrix \n",
    "        yxw = numpy.dot (yx, self.w) # s—by-1 matrix \n",
    "        vecl = numpy.exp(yxw) # s—by-1 matrix \n",
    "        vec2 = numpy.divide(yx, 1+vecl) # s—by—d matrix \n",
    "        g = -numpy.sum(vec2,axis=0).reshape(self.d, 1) # d—by-1 matrix return g \n",
    "        return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Server Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server : \n",
    "    def __init__ (self , m, n, d): \n",
    "        self.m = m # number of worker nodes \n",
    "        self.n = n # number of training samples \n",
    "        self.d = d # number of features \n",
    "        self.w = numpy.zeros ((d, 1)) # # d—by-1 model parameter vector \n",
    "        self.g = numpy.zeros ((d, 1)) # d—by-1 gradient \n",
    "        self.v = numpy.zeros ((d, 1)) # d—by —1 momentum \n",
    "        self. loss = 0 # loss function value \n",
    "        self. obj = 0 # objective function value \n",
    "        \n",
    "    def broadcast ( self ) : \n",
    "        return self.w \n",
    "\n",
    "# Sum the gradients and loss functions evaluated by the workers \n",
    "# Args: \n",
    "    # grads : a list of d—by-1 vectors\n",
    "    # losses : a list of scalars \n",
    "    \n",
    "    def aggregate (self , grads , losses ) : \n",
    "        self.g = numpy.zeros (( self.d, 1)) \n",
    "        self.loss = 0 \n",
    "        for k in range( self.m) : \n",
    "            #print(self.g)\n",
    "            #print(\"----\")\n",
    "            #print(grads[k])\n",
    "            self.g += grads[k] \n",
    "            self.loss += losses[k] \n",
    "            \n",
    "# Compute the gradient (from the loss and regularization ) \n",
    "    def gradient ( self , lam) : \n",
    "        self.g = self.g / self.n + lam * self.w \n",
    "        \n",
    "# Compute the objective function (sum of loss and regularization ) \n",
    "    def objective ( self , lam) : \n",
    "        reg = lam / 2 * numpy.sum( self.w * self.w) \n",
    "        self.obj = self.loss / self.n + reg \n",
    "        return self.obj \n",
    "    \n",
    "# Update the model parameters using accelerated gradient descent \n",
    "# Args: \n",
    "    # alpha: learning rate (step size ) \n",
    "    # beta : momentum parameter \n",
    "    def agd( self , alpha , beta) : \n",
    "        self.v *= beta \n",
    "        self.v += self.g \n",
    "        self.w -= alpha * self.v \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "# Create a server and m worker nodes \n",
    "def create_server_workers (m, x, y) : \n",
    "    n, d = x. shape \n",
    "    s = math. floor (n / m) \n",
    "    server = Server (m, n, d) \n",
    "    workers = [] \n",
    "    for i in range (m) : \n",
    "        indices = list (range(i*s , (i+1)*s)) \n",
    "        worker = Worker(x [ indices , :] , y [ indices , :]) \n",
    "        workers . append ( worker ) \n",
    "    return server , workers \n",
    "\n",
    " \n",
    "m = 4 # number of worker nodes \n",
    "server , workers = create_server_workers (m, x_train , y_train ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective function value = 0.6897659748011163\n",
      "Objective function value = 0.6775401123908128\n",
      "Objective function value = 0.6561613161435644\n",
      "Objective function value = 0.6295661257206712\n",
      "Objective function value = 0.6016290311027173\n",
      "Objective function value = 0.5754074408484188\n",
      "Objective function value = 0.5527686087790677\n",
      "Objective function value = 0.5344392387890108\n",
      "Objective function value = 0.520318129596414\n",
      "Objective function value = 0.5098424275394172\n"
     ]
    }
   ],
   "source": [
    "lam = 1E-6 # regularization parameter \n",
    "alpha = 1E-1 # learning rate \n",
    "beta = 0.9 # momentum parameter \n",
    "max_epoch = 10 # number of epochs \n",
    "for t in range (max_epoch) : \n",
    "    # step 1: broadcast \n",
    "    w = server.broadcast() \n",
    "    for i in range (m) : \n",
    "        workers[ i ].set_param (w) \n",
    "\n",
    "    # step 2: workers' local computations \n",
    "    grads = [] \n",
    "    losses = [] \n",
    "    q=1\n",
    "    for i in range (m) : \n",
    "        g = workers[i].gradient(q)\n",
    "        grads.append(g) \n",
    "        l = workers[i].loss() \n",
    "        losses.append( l ) \n",
    "    #print(\"grad\" + str(grads))\n",
    "    # step 3: aggregate the workers ' outputs \n",
    "    server.aggregate( grads , losses) \n",
    "\n",
    "    # step 4: server update the model parameters \n",
    "    server.gradient(lam) # compute gradient \n",
    "    obj = server.objective (lam) # compute compute objective function \n",
    "    print ('Objective function value = ' + str (obj) ) \n",
    "    server.agd (alpha , beta) # updates the model parameters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the objective function value\n",
    "# Inputs:\n",
    "#     w: d-by-1 matrix\n",
    "#     x: n-by-d matrix\n",
    "#     y: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "# Return:\n",
    "#     objective function value (scalar)\n",
    "def objective(w, x, y, lam):\n",
    "    n, d = x.shape\n",
    "    yx = numpy.multiply(y, x) # n-by-d matrix\n",
    "    yxw = numpy.dot(yx, w) # n-by-1 matrix\n",
    "    vec1 = numpy.exp(-yxw) # n-by-1 matrix\n",
    "    vec2 = numpy.log(1 + vec1) # n-by-1 matrix\n",
    "    loss = numpy.mean(vec2) # scalar\n",
    "    reg = lam / 2 * numpy.sum(w * w) # scalar\n",
    "    return loss + reg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial objective function value = 0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "# initialize w\n",
    "d = x_train.shape[1]\n",
    "w = numpy.zeros((d, 1))\n",
    "\n",
    "# evaluate the objective function value at w\n",
    "lam = 1E-6\n",
    "objval0 = objective(w, x_train, y_train, lam)\n",
    "print('Initial objective function value = ' + str(objval0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Numerical optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient at $w$ is $g = - \\frac{1}{n} \\sum_{i=1}^n \\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the gradient\n",
    "# Inputs:\n",
    "#     w: d-by-1 matrix\n",
    "#     x: n-by-d matrix\n",
    "#     y: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "# Return:\n",
    "#     g: g: d-by-1 matrix, full gradient\n",
    "def gradient(w, x, y, lam):\n",
    "    n, d = x.shape\n",
    "    yx = numpy.multiply(y, x) # n-by-d matrix\n",
    "    yxw = numpy.dot(yx, w) # n-by-1 matrix\n",
    "    vec1 = numpy.exp(yxw) # n-by-1 matrix\n",
    "    vec2 = numpy.divide(yx, 1+vec1) # n-by-d matrix\n",
    "    vec3 = -numpy.mean(vec2, axis=0).reshape(d, 1) # d-by-1 matrix\n",
    "    g = vec3 + lam * w\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent for solving logistic regression\n",
    "# Inputs:\n",
    "#     x: n-by-d matrix\n",
    "#     y: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     stepsize: scalar\n",
    "#     max_iter: integer, the maximal iterations\n",
    "#     w: d-by-1 matrix, initialization of w\n",
    "# Return:\n",
    "#     w: d-by-1 matrix, the solution\n",
    "#     objvals: a record of each iteration's objective value\n",
    "def grad_descent(x, y, lam, stepsize, max_iter=100, w=None):\n",
    "    n, d = x.shape\n",
    "    objvals = numpy.zeros(max_iter) # store the objective values\n",
    "    if w is None:\n",
    "        w = numpy.zeros((d, 1)) # zero initialization\n",
    "    \n",
    "    for t in range(max_iter):\n",
    "        objval = objective(w, x, y, lam)\n",
    "        objvals[t] = objval\n",
    "        print('Objective value at t=' + str(t) + ' is ' + str(objval))\n",
    "        g = gradient(w, x, y, lam)\n",
    "        w -= stepsize * g\n",
    "    \n",
    "    return w, objvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective value at t=0 is 0.6931471805599453\n",
      "Objective value at t=1 is 0.5929443919757862\n",
      "Objective value at t=2 is 0.5516037061665903\n",
      "Objective value at t=3 is 0.5293145114381133\n",
      "Objective value at t=4 is 0.5154463440683746\n",
      "Objective value at t=5 is 0.5061065052355512\n",
      "Objective value at t=6 is 0.49950067219294686\n",
      "Objective value at t=7 is 0.49467040558974085\n",
      "Objective value at t=8 is 0.4910513542048101\n",
      "Objective value at t=9 is 0.48828823173432295\n",
      "Objective value at t=10 is 0.4861463994615091\n",
      "Objective value at t=11 is 0.4844652386794547\n",
      "Objective value at t=12 is 0.48313167133135065\n",
      "Objective value at t=13 is 0.4820642537341396\n",
      "Objective value at t=14 is 0.4812031978683424\n",
      "Objective value at t=15 is 0.4805038922909274\n",
      "Objective value at t=16 is 0.4799325756508955\n",
      "Objective value at t=17 is 0.4794633788304112\n",
      "Objective value at t=18 is 0.4790762614294054\n",
      "Objective value at t=19 is 0.4787555464100609\n",
      "Objective value at t=20 is 0.4784888629534222\n",
      "Objective value at t=21 is 0.4782663729074242\n",
      "Objective value at t=22 is 0.47808019742405555\n",
      "Objective value at t=23 is 0.4779239869737724\n",
      "Objective value at t=24 is 0.4777925954165538\n",
      "Objective value at t=25 is 0.4776818305167643\n",
      "Objective value at t=26 is 0.4775882612500188\n",
      "Objective value at t=27 is 0.4775090677421779\n",
      "Objective value at t=28 is 0.47744192352001774\n",
      "Objective value at t=29 is 0.4773849024706257\n",
      "Objective value at t=30 is 0.47733640485236384\n",
      "Objective value at t=31 is 0.4772950981085867\n",
      "Objective value at t=32 is 0.4772598692650672\n",
      "Objective value at t=33 is 0.47722978645224423\n",
      "Objective value at t=34 is 0.4772040676596348\n",
      "Objective value at t=35 is 0.47718205525510016\n",
      "Objective value at t=36 is 0.477163195123753\n",
      "Objective value at t=37 is 0.47714701952704025\n",
      "Objective value at t=38 is 0.4771331329713819\n",
      "Objective value at t=39 is 0.47712120052183465\n",
      "Objective value at t=40 is 0.47711093810998983\n",
      "Objective value at t=41 is 0.47710210447439727\n",
      "Objective value at t=42 is 0.47709449444196544\n",
      "Objective value at t=43 is 0.47708793331435667\n",
      "Objective value at t=44 is 0.4770822721676056\n",
      "Objective value at t=45 is 0.4770773839085479\n",
      "Objective value at t=46 is 0.47707315996004024\n",
      "Objective value at t=47 is 0.4770695074698579\n",
      "Objective value at t=48 is 0.47706634695669786\n",
      "Objective value at t=49 is 0.4770636103217893\n",
      "Objective value at t=50 is 0.4770612391669017\n",
      "Objective value at t=51 is 0.47705918336960307\n",
      "Objective value at t=52 is 0.47705739987486856\n",
      "Objective value at t=53 is 0.4770558516689417\n",
      "Objective value at t=54 is 0.4770545069069508\n",
      "Objective value at t=55 is 0.47705333817042855\n",
      "Objective value at t=56 is 0.4770523218347286\n",
      "Objective value at t=57 is 0.47705143752953516\n",
      "Objective value at t=58 is 0.4770506676783216\n",
      "Objective value at t=59 is 0.4770499971048465\n",
      "Objective value at t=60 is 0.4770494126966285\n",
      "Objective value at t=61 is 0.47704890311690623\n",
      "Objective value at t=62 is 0.47704845855789285\n",
      "Objective value at t=63 is 0.4770480705292377\n",
      "Objective value at t=64 is 0.4770477316765303\n",
      "Objective value at t=65 is 0.4770474356254632\n",
      "Objective value at t=66 is 0.47704717684792886\n",
      "Objective value at t=67 is 0.47704695054687946\n",
      "Objective value at t=68 is 0.4770467525572536\n",
      "Objective value at t=69 is 0.47704657926066873\n",
      "Objective value at t=70 is 0.477046427511916\n",
      "Objective value at t=71 is 0.4770462945755866\n",
      "Objective value at t=72 is 0.477046178071395\n",
      "Objective value at t=73 is 0.4770460759269756\n",
      "Objective value at t=74 is 0.4770459863371095\n",
      "Objective value at t=75 is 0.4770459077284784\n",
      "Objective value at t=76 is 0.4770458387291838\n",
      "Objective value at t=77 is 0.4770457781423668\n",
      "Objective value at t=78 is 0.477045724923366\n",
      "Objective value at t=79 is 0.47704567815992666\n",
      "Objective value at t=80 is 0.4770456370550442\n",
      "Objective value at t=81 is 0.4770456009120828\n",
      "Objective value at t=82 is 0.47704556912185914\n",
      "Objective value at t=83 is 0.4770455411514286\n",
      "Objective value at t=84 is 0.4770455165343394\n",
      "Objective value at t=85 is 0.4770454948621638\n",
      "Objective value at t=86 is 0.4770454757771297\n",
      "Objective value at t=87 is 0.47704545896571054\n",
      "Objective value at t=88 is 0.4770454441530431\n",
      "Objective value at t=89 is 0.4770454310980652\n",
      "Objective value at t=90 is 0.4770454195892786\n",
      "Objective value at t=91 is 0.47704540944105334\n",
      "Objective value at t=92 is 0.47704540049040467\n",
      "Objective value at t=93 is 0.4770453925941795\n",
      "Objective value at t=94 is 0.47704538562659954\n",
      "Objective value at t=95 is 0.47704537947711523\n",
      "Objective value at t=96 is 0.47704537404852954\n",
      "Objective value at t=97 is 0.477045369255358\n",
      "Objective value at t=98 is 0.4770453650223937\n",
      "Objective value at t=99 is 0.47704536128345154\n"
     ]
    }
   ],
   "source": [
    "lam = 1E-6\n",
    "stepsize = 1.0\n",
    "w, objvals_gd = grad_descent(x_train, y_train, lam, stepsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Stochastic gradient descent (SGD)\n",
    "\n",
    "Define $Q_i (w) = \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $.\n",
    "\n",
    "The stochastic gradient at $w$ is $g_i = \\frac{\\partial Q_i }{ \\partial w} = -\\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the objective Q_i and the gradient of Q_i\n",
    "# Inputs:\n",
    "#     w: d-by-1 matrix\n",
    "#     xi: 1-by-d matrix\n",
    "#     yi: scalar\n",
    "#     lam: scalar, the regularization parameter\n",
    "# Return:\n",
    "#     obj: scalar, the objective Q_i\n",
    "#     g: d-by-1 matrix, gradient of Q_i\n",
    "def stochastic_objective_gradient(w, xi, yi, lam):\n",
    "    d = xi.shape[0]\n",
    "    yx = yi * xi # 1-by-d matrix\n",
    "    yxw = float(numpy.dot(yx, w)) # scalar\n",
    "    \n",
    "    # calculate objective function Q_i\n",
    "    loss = numpy.log(1 + numpy.exp(-yxw)) # scalar\n",
    "    reg = lam / 2 * numpy.sum(w * w) # scalar\n",
    "    obj = loss + reg\n",
    "    \n",
    "    # calculate stochastic gradient\n",
    "    g_loss = -yx.T / (1 + numpy.exp(yxw)) # d-by-1 matrix\n",
    "    g = g_loss + lam * w # d-by-1 matrix\n",
    "    \n",
    "    return obj, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD for solving logistic regression\n",
    "# Inputs:\n",
    "#     x: n-by-d matrix\n",
    "#     y: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     stepsize: scalar\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "#     w: d-by-1 matrix, initialization of w\n",
    "# Return:\n",
    "#     w: the solution\n",
    "#     objvals: record of each iteration's objective value\n",
    "def sgd(x, y, lam, stepsize, max_epoch=100, w=None):\n",
    "    n, d = x.shape\n",
    "    objvals = numpy.zeros(max_epoch) # store the objective values\n",
    "    if w is None:\n",
    "        w = numpy.zeros((d, 1)) # zero initialization\n",
    "    \n",
    "    for t in range(max_epoch):\n",
    "        # randomly shuffle the samples\n",
    "        rand_indices = numpy.random.permutation(n)\n",
    "        x_rand = x[rand_indices, :]\n",
    "        y_rand = y[rand_indices, :]\n",
    "        \n",
    "        objval = 0 # accumulate the objective values\n",
    "        for i in range(n):\n",
    "            xi = x_rand[i, :] # 1-by-d matrix\n",
    "            yi = float(y_rand[i, :]) # scalar\n",
    "            obj, g = stochastic_objective_gradient(w, xi, yi, lam)\n",
    "            objval += obj\n",
    "            w -= stepsize * g\n",
    "        \n",
    "        stepsize *= 0.9 # decrease step size\n",
    "        objval /= n\n",
    "        objvals[t] = objval\n",
    "        print('Objective value at epoch t=' + str(t) + ' is ' + str(objval))\n",
    "    \n",
    "    return w, objvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective value at epoch t=0 is 0.52809130798053\n",
      "Objective value at epoch t=1 is 0.5321133061485736\n",
      "Objective value at epoch t=2 is 0.5143560880311044\n",
      "Objective value at epoch t=3 is 0.5149895098636783\n",
      "Objective value at epoch t=4 is 0.5114225054744105\n",
      "Objective value at epoch t=5 is 0.49871481189998923\n",
      "Objective value at epoch t=6 is 0.5115396747466047\n",
      "Objective value at epoch t=7 is 0.5072164939740491\n",
      "Objective value at epoch t=8 is 0.4953700925997171\n",
      "Objective value at epoch t=9 is 0.49457738321473066\n",
      "Objective value at epoch t=10 is 0.49579711341449806\n",
      "Objective value at epoch t=11 is 0.4964601584689119\n",
      "Objective value at epoch t=12 is 0.49266703867044725\n",
      "Objective value at epoch t=13 is 0.49193309158790705\n",
      "Objective value at epoch t=14 is 0.4931084540715684\n",
      "Objective value at epoch t=15 is 0.489913384373484\n",
      "Objective value at epoch t=16 is 0.4873226143615006\n",
      "Objective value at epoch t=17 is 0.48699195098891107\n",
      "Objective value at epoch t=18 is 0.4831165394607236\n",
      "Objective value at epoch t=19 is 0.4853566412804967\n",
      "Objective value at epoch t=20 is 0.48519622182456146\n",
      "Objective value at epoch t=21 is 0.48430235980789677\n",
      "Objective value at epoch t=22 is 0.4836065086968899\n",
      "Objective value at epoch t=23 is 0.48307234323931136\n",
      "Objective value at epoch t=24 is 0.48266994533144014\n",
      "Objective value at epoch t=25 is 0.48173536952399687\n",
      "Objective value at epoch t=26 is 0.48165171883112634\n",
      "Objective value at epoch t=27 is 0.4811502883714791\n",
      "Objective value at epoch t=28 is 0.4807756947371724\n",
      "Objective value at epoch t=29 is 0.48057388126869555\n",
      "Objective value at epoch t=30 is 0.48025997465946596\n",
      "Objective value at epoch t=31 is 0.47979587463381446\n",
      "Objective value at epoch t=32 is 0.4792315489589802\n",
      "Objective value at epoch t=33 is 0.47936225586610265\n",
      "Objective value at epoch t=34 is 0.4789922455085655\n",
      "Objective value at epoch t=35 is 0.4789247788511408\n",
      "Objective value at epoch t=36 is 0.47872180226841576\n",
      "Objective value at epoch t=37 is 0.4785772969824061\n",
      "Objective value at epoch t=38 is 0.4784452643940429\n",
      "Objective value at epoch t=39 is 0.47828972807324155\n",
      "Objective value at epoch t=40 is 0.4781719317974909\n",
      "Objective value at epoch t=41 is 0.4780515193820271\n",
      "Objective value at epoch t=42 is 0.47795659403971946\n",
      "Objective value at epoch t=43 is 0.4778642702269043\n",
      "Objective value at epoch t=44 is 0.47779099712493617\n",
      "Objective value at epoch t=45 is 0.4777177545537509\n",
      "Objective value at epoch t=46 is 0.4776512534668578\n",
      "Objective value at epoch t=47 is 0.4775898926231676\n",
      "Objective value at epoch t=48 is 0.4775369909191774\n",
      "Objective value at epoch t=49 is 0.47748898500000475\n",
      "Objective value at epoch t=50 is 0.4774449109378235\n",
      "Objective value at epoch t=51 is 0.4774042592886638\n",
      "Objective value at epoch t=52 is 0.4773682146195281\n",
      "Objective value at epoch t=53 is 0.4773360553101729\n",
      "Objective value at epoch t=54 is 0.47730805773252694\n",
      "Objective value at epoch t=55 is 0.47728151400763175\n",
      "Objective value at epoch t=56 is 0.47725664229390213\n",
      "Objective value at epoch t=57 is 0.47723701089632803\n",
      "Objective value at epoch t=58 is 0.47721784320561594\n",
      "Objective value at epoch t=59 is 0.47720075623064867\n",
      "Objective value at epoch t=60 is 0.4771851221770642\n",
      "Objective value at epoch t=61 is 0.47717134802939376\n",
      "Objective value at epoch t=62 is 0.47715880093440993\n",
      "Objective value at epoch t=63 is 0.47714755808491344\n",
      "Objective value at epoch t=64 is 0.4771373988639285\n",
      "Objective value at epoch t=65 is 0.47712810853375176\n",
      "Objective value at epoch t=66 is 0.4771201378083347\n",
      "Objective value at epoch t=67 is 0.47711268427221004\n",
      "Objective value at epoch t=68 is 0.47710597367918606\n",
      "Objective value at epoch t=69 is 0.47710003103927195\n",
      "Objective value at epoch t=70 is 0.47709456869355005\n",
      "Objective value at epoch t=71 is 0.47708977348701137\n",
      "Objective value at epoch t=72 is 0.47708538764830744\n",
      "Objective value at epoch t=73 is 0.47708144400307706\n",
      "Objective value at epoch t=74 is 0.4770778908260698\n",
      "Objective value at epoch t=75 is 0.4770747064497263\n",
      "Objective value at epoch t=76 is 0.47707183994703\n",
      "Objective value at epoch t=77 is 0.4770692553588919\n",
      "Objective value at epoch t=78 is 0.477066927070147\n",
      "Objective value at epoch t=79 is 0.47706482769060143\n",
      "Objective value at epoch t=80 is 0.47706294032237295\n",
      "Objective value at epoch t=81 is 0.47706124538025396\n",
      "Objective value at epoch t=82 is 0.4770597177983495\n",
      "Objective value at epoch t=83 is 0.47705834067895536\n",
      "Objective value at epoch t=84 is 0.4770571061462452\n",
      "Objective value at epoch t=85 is 0.47705599057441567\n",
      "Objective value at epoch t=86 is 0.4770549884679477\n",
      "Objective value at epoch t=87 is 0.4770540864284776\n",
      "Objective value at epoch t=88 is 0.47705327428674466\n",
      "Objective value at epoch t=89 is 0.47705254337571434\n",
      "Objective value at epoch t=90 is 0.4770518842899294\n",
      "Objective value at epoch t=91 is 0.4770512933008911\n",
      "Objective value at epoch t=92 is 0.47705076015522324\n",
      "Objective value at epoch t=93 is 0.4770502804358812\n",
      "Objective value at epoch t=94 is 0.47704984912162535\n",
      "Objective value at epoch t=95 is 0.47704946056440506\n",
      "Objective value at epoch t=96 is 0.4770491110076305\n",
      "Objective value at epoch t=97 is 0.47704879618755774\n",
      "Objective value at epoch t=98 is 0.4770485129426697\n",
      "Objective value at epoch t=99 is 0.47704825803814727\n"
     ]
    }
   ],
   "source": [
    "lam = 1E-6\n",
    "stepsize = 0.1\n",
    "w, objvals_sgd = sgd(x_train, y_train, lam, stepsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compare GD with SGD\n",
    "\n",
    "Plot objective function values against epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "\n",
    "epochs_gd = range(len(objvals_gd))\n",
    "epochs_sgd = range(len(objvals_sgd))\n",
    "\n",
    "line0, = plt.plot(epochs_gd, objvals_gd, '--b', LineWidth=4)\n",
    "line1, = plt.plot(epochs_sgd, objvals_sgd, '-r', LineWidth=2)\n",
    "plt.xlabel('Epochs', FontSize=20)\n",
    "plt.ylabel('Objective Value', FontSize=20)\n",
    "plt.xticks(FontSize=16)\n",
    "plt.yticks(FontSize=16)\n",
    "plt.legend([line0, line1], ['GD', 'SGD'], fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('compare_gd_sgd.pdf', format='pdf', dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class label\n",
    "# Inputs:\n",
    "#     w: d-by-1 matrix\n",
    "#     X: m-by-d matrix\n",
    "# Return:\n",
    "#     f: m-by-1 matrix, the predictions\n",
    "def predict(w, X):\n",
    "    xw = numpy.dot(X, w)\n",
    "    f = numpy.sign(xw)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classification error is 0.21951219512195122\n"
     ]
    }
   ],
   "source": [
    "# evaluate training error\n",
    "f_train = predict(w, x_train)\n",
    "diff = numpy.abs(f_train - y_train) / 2\n",
    "error_train = numpy.mean(diff)\n",
    "print('Training classification error is ' + str(error_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test classification error is 0.22875816993464052\n"
     ]
    }
   ],
   "source": [
    "# evaluate test error\n",
    "f_test = predict(w, x_test)\n",
    "diff = numpy.abs(f_test - y_test) / 2\n",
    "error_test = numpy.mean(diff)\n",
    "print('Test classification error is ' + str(error_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
